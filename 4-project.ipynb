{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project MLE - Spam filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Collect a dataset of emails labeled as spam or not spam.\n",
    "Preprocess the emails by removing stop words, stemming, and converting text into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exampl sentenc . contain multipl sentenc !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/daniloneves/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daniloneves/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daniloneves/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.data.path.append(\"nltk_data\")\n",
    "\n",
    "# Download stop words and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Convert numpy array to list of strings\n",
    "    if isinstance(text, np.ndarray):\n",
    "        text = text.tolist()\n",
    "    \n",
    "    # Convert list of strings to a single string\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # Convert to lowercase and remove non-alphanumeric characters\n",
    "    text = text.lower().replace('[^\\w\\s]','')\n",
    "\n",
    "    # Tokenize and remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stem words\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence. It contains multiple sentences!\"\n",
    "preprocessed_text = preprocess(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "num_samples = 1000\n",
    "num_features = 5000\n",
    "X = np.random.rand(num_samples, num_features)\n",
    "y = np.random.randint(2, size=num_samples)\n",
    "\n",
    "# Save data to files\n",
    "np.save('X_train.npy', X)\n",
    "np.save('y_train.npy', y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Create a statistical model, such as Naive Bayes or logistic regression, to predict whether an email is spam or not spam.\n",
    "Use MLE to estimate the model parameters that maximize the likelihood of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m X_train, y_train\n\u001b[1;32m     16\u001b[0m \u001b[39m# Load preprocessed data and labels\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m X_train, y_train \u001b[39m=\u001b[39m load_data()\n\u001b[1;32m     19\u001b[0m \u001b[39m# Create bag-of-words vectorizer\u001b[39;00m\n\u001b[1;32m     20\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39my_train.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Convert numpy array to list of strings\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m X_train \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m X_train]\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m X_train, y_train\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39my_train.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Convert numpy array to list of strings\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m X_train \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m X_train]\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m X_train, y_train\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    # Load preprocessed data and labels\n",
    "    X_train = np.load('X_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "\n",
    "    # Convert numpy array to list of strings\n",
    "    X_train = np.load('X_train.npy', allow_pickle=True) \n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "\n",
    "# Load preprocessed data and labels\n",
    "X_train, y_train = load_data()\n",
    "\n",
    "# Create bag-of-words vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert text to bag-of-words features\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Train Naive Bayes model with MLE\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_bow, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Evaluate the trained model on a held-out test set to measure its performance.\n",
    "Tune the model's hyperparameters, such as regularization strength or feature selection, to improve its accuracy.\n",
    "Sample code for model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'X_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[39m# Load preprocessed data and labels\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X, y \u001b[39m=\u001b[39m load_data()\n\u001b[1;32m      7\u001b[0m \u001b[39m# Split data into training and test sets\u001b[39;00m\n\u001b[1;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m():\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Load preprocessed data and labels\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mX_train.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m     y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39my_train.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m X_train, y_train\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_train.npy'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load preprocessed data and labels\n",
    "X, y = load_data()\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Convert text to bag-of-words features\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Naive Bayes model with MLE\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Evaluate model on test set\n",
    "y_pred = clf.predict(X_test_bow)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Use the trained model to predict whether new, unseen emails are spam or not spam.\n",
    "Preprocess the new emails in the same way as the training data before making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess new email text\n",
    "new_email = \"Get rich quick! Earn thousands of dollars a day!\"\n",
    "preprocessed_email = preprocess(new_email)\n",
    "\n",
    "# Convert preprocessed text to bag-of-words features\n",
    "new_email_b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
