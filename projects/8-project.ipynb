{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8: Apply key techniques employed in building deep learning architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Business Understanding: Classify handwritten digits\n",
    "\n",
    "# Data Understanding: Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Data Preparation: Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reshape images to match the input shape expected by EfficientNet\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Data Augmentation: Generate additional training data\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Transfer Learning: Load the pre-trained EfficientNetB0 model\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(28, 28, 1))\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers for digit classification\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=10,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluation: Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Model Deployment: Make predictions on new, unseen data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Hyperparameter Tuning: Grid Search\n",
    "param_grid = {'optimizer': ['adam', 'sgd'], 'learning_rate': [0.001, 0.01]}\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model.compile(optimizer=params['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "              steps_per_epoch=len(X_train) / 32, epochs=10,\n",
    "              validation_data=(X_val, y_val))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Parameters: {params}')\n",
    "    print(f'Test loss: {loss:.4f}')\n",
    "    print(f'Test accuracy: {accuracy:.4f}')\n",
    "    print()\n",
    "\n",
    "# Model Evaluation: Cross-Validation\n",
    "k = 5\n",
    "X = np.concatenate((X_train, X_val), axis=0)\n",
    "y = np.concatenate((y_train, y_val), axis=0)\n",
    "folds = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "for fold, (train_indices, val_indices) in enumerate(folds.split(X)):\n",
    "    X_train_fold = X[train_indices]\n",
    "    y_train_fold = y[train_indices]\n",
    "    X_val_fold = X[val_indices]\n",
    "    y_val_fold = y[val_indices]\n",
    "    model.fit(datagen.flow(X_train_fold, y_train_fold, batch_size=32),\n",
    "              steps_per_epoch=len(X_train_fold) / 32, epochs=10,\n",
    "              validation_data=(X_val_fold, y_val_fold))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Fold {fold+1}:')\n",
    "    print(f'Test loss: {loss:.4f}')\n",
    "    print(f'Test accuracy: {accuracy:.4f}')\n",
    "    print()\n",
    "\n",
    "# Model Deployment: Save the trained model for future use\n",
    "model.save('mnist_classification_model.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "The activation function 'relu' is used in the custom layers of the model for introducing non-linearity.\n",
    "\n",
    "Convolutional operations are implicitly performed by the EfficientNetB0 model, which has convolutional layers that automatically learn relevant features from the input images.\n",
    "\n",
    "The code focuses on image classification, so recurrent connections are not relevant in this specific context. Recurrent connections are typically used for sequential data such as text or time series.\n",
    "\n",
    "Regularization techniques, such as dropout and batch normalization, are not explicitly applied in this code snippet. However, you can incorporate them by adding Dropout and BatchNormalization layers to the custom layers of the model.\n",
    "\n",
    "The code utilizes the Adam optimization algorithm with a learning rate of 0.001. Hyperparameter tuning can be performed by exploring different optimizers and learning rates using techniques like grid search, as shown in the example.\n",
    "\n",
    "Transfer learning is applied by using the pre-trained EfficientNetB0 model, which has been trained on the ImageNet dataset. The pre-trained weights are frozen, and only the custom layers on top are trained for the specific task of digit classification.\n",
    "\n",
    "Model evaluation is performed by calculating the test loss, accuracy, and generating a classification report to assess the performance of the model on the test set.\n",
    "\n",
    "Model deployment is demonstrated by saving the trained model to a file for future use.\n",
    "\n",
    "Note that the code snippets provided are for illustrative purposes and may require further modifications based on your specific use case or dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
