{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Variation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we first import the necessary libraries, including pandas for data manipulation, numpy for numerical operations, matplotlib for data visualization, and scikit-learn for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the dataset from the UCI Machine Learning Repository, and perform data cleaning and preparation by removing any missing or invalid data, dropping duplicate records or transactions, and converting categorical variables into numerical variables using one-hot encoding. We then normalize or standardize the data to eliminate any differences in scale or range between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1    536365     71053                  WHITE METAL LANTERN         6   \n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "\n",
      "          InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0 2010-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
      "1 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "2 2010-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
      "3 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "4 2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel(\"data//Online_Retail.xlsx\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Data cleaning and preparation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mdropna(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m# Drop any missing or invalid data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df\u001b[39m.\u001b[39;49mdrop_duplicates(inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m# Drop any duplicate records or transactions\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/pandas/core/frame.py:6672\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6670\u001b[0m inplace \u001b[39m=\u001b[39m validate_bool_kwarg(inplace, \u001b[39m\"\u001b[39m\u001b[39minplace\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6671\u001b[0m ignore_index \u001b[39m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[39m\"\u001b[39m\u001b[39mignore_index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 6672\u001b[0m duplicated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mduplicated(subset, keep\u001b[39m=\u001b[39;49mkeep)\n\u001b[1;32m   6674\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m[\u001b[39m-\u001b[39mduplicated]\n\u001b[1;32m   6675\u001b[0m \u001b[39mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/pandas/core/frame.py:6814\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6812\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6813\u001b[0m     vals \u001b[39m=\u001b[39m (col\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m name, col \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m subset)\n\u001b[0;32m-> 6814\u001b[0m     labels, shape \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(f, vals)))\n\u001b[1;32m   6816\u001b[0m     ids \u001b[39m=\u001b[39m get_group_index(\n\u001b[1;32m   6817\u001b[0m         labels,\n\u001b[1;32m   6818\u001b[0m         \u001b[39m# error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6822\u001b[0m         xnull\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   6823\u001b[0m     )\n\u001b[1;32m   6824\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/pandas/core/frame.py:6782\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[0;34m(vals)\u001b[0m\n\u001b[1;32m   6781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(vals) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[np\u001b[39m.\u001b[39mndarray, \u001b[39mint\u001b[39m]:\n\u001b[0;32m-> 6782\u001b[0m     labels, shape \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49mfactorize(vals, size_hint\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m   6783\u001b[0m     \u001b[39mreturn\u001b[39;00m labels\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), \u001b[39mlen\u001b[39m(shape)\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/pandas/core/algorithms.py:822\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[39m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 822\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[1;32m    823\u001b[0m         values,\n\u001b[1;32m    824\u001b[0m         na_sentinel\u001b[39m=\u001b[39;49mna_sentinel_arg,\n\u001b[1;32m    825\u001b[0m         size_hint\u001b[39m=\u001b[39;49msize_hint,\n\u001b[1;32m    826\u001b[0m     )\n\u001b[1;32m    828\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m         \u001b[39m# TODO: Can remove when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Files/Projects/pytorch-test/env/lib/python3.8/site-packages/pandas/core/algorithms.py:578\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    575\u001b[0m hash_klass, values \u001b[39m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    577\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[0;32m--> 578\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39;49mfactorize(\n\u001b[1;32m    579\u001b[0m     values,\n\u001b[1;32m    580\u001b[0m     na_sentinel\u001b[39m=\u001b[39;49mna_sentinel,\n\u001b[1;32m    581\u001b[0m     na_value\u001b[39m=\u001b[39;49mna_value,\n\u001b[1;32m    582\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[1;32m    583\u001b[0m     ignore_na\u001b[39m=\u001b[39;49mignore_na,\n\u001b[1;32m    584\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[39m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[1;32m    587\u001b[0m uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data cleaning and preparation\n",
    "df.dropna(inplace=True) # Drop any missing or invalid data\n",
    "df.drop_duplicates(inplace=True) # Drop any duplicate records or transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables into numerical variables using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# Verify null values\n",
    "null_columns = df.columns[df.isnull().any()]\n",
    "print(df[null_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Normalize or standardize the data to eliminate any differences in scale or range between the variables\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df.drop(['InvoiceNo', 'StockCode', 'InvoiceDate', 'Quantity', 'UnitPrice', 'CustomerID', 'Country'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA to reduce dimensionality of dataset\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform clustering analysis, we use the k-means clustering algorithm, which partitions the data into k distinct clusters based on the similarity of the data points. We evaluate the performance of the algorithm using the silhouette score, which measures the similarity of a data point to its own cluster compared to other clusters, and ranges from -1 to 1, with higher values indicating better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "silhouette_scores = []\n",
    "k_values = range(2, 11)\n",
    "\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_pca)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_scores.append(silhouette_score(df_pca, labels))\n",
    "\n",
    "# Plot the silhouette scores to determine the optimal number of clusters\n",
    "plt.plot(k_values, silhouette_scores)\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for k-means Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Fit the k-means clustering algorithm with the optimal number of clusters (k=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(df_pca)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the results of the clustering analysis using scatter plots, with each data point colored according to its assigned cluster. We identify customer segments based on the cluster means, and these segments can be used to develop targeted marketing strategies and improve customer satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of clustering results\n",
    "plt.scatter(df_pca[:, 0], df_pca[:, 1], c=labels)\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('Customer Segmentation using k-means Clustering')\n",
    "plt.show()\n",
    "\n",
    "# Identify customer segments\n",
    "df['Cluster'] = labels\n",
    "cluster_means = df.groupby('Cluster').mean()\n",
    "print(cluster_means)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
